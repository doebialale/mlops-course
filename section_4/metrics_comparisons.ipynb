{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Machine Learning Models in Production\n",
    "\n",
    "This lecture demonstrates various strategies for comparing the performance of different sklearn models in a production environment. We'll cover the following approaches:\n",
    "\n",
    "1. A/B Testing\n",
    "2. G-Test\n",
    "3. Multi-armed Bandit\n",
    "4. Defining a metric-only dataset\n",
    "\n",
    "We'll use a simple dataset and two different sklearn models to illustrate these concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import chi2_contingency\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation and Model Training\n",
    "\n",
    "First, let's generate a synthetic dataset and train two different models: Logistic Regression and Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "X, y = make_classification(n_samples=10000, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "lr_model = LogisticRegression(random_state=42)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Train Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "lr_preds = lr_model.predict(X_test)\n",
    "rf_preds = rf_model.predict(X_test)\n",
    "\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_score(y_test, lr_preds):.4f}\")\n",
    "print(f\"Random Forest Accuracy: {accuracy_score(y_test, rf_preds):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. A/B Testing\n",
    "\n",
    "A/B testing is a method of comparing two versions of a model to determine which one performs better. In this case, we'll compare the Logistic Regression model (A) with the Random Forest model (B)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ab_test(model_a_preds, model_b_preds, true_labels):\n",
    "    model_a_accuracy = accuracy_score(true_labels, model_a_preds)\n",
    "    model_b_accuracy = accuracy_score(true_labels, model_b_preds)\n",
    "    \n",
    "    print(f\"Model A Accuracy: {model_a_accuracy:.4f}\")\n",
    "    print(f\"Model B Accuracy: {model_b_accuracy:.4f}\")\n",
    "    \n",
    "    if model_a_accuracy > model_b_accuracy:\n",
    "        print(\"Model A (Logistic Regression) performs better.\")\n",
    "    elif model_b_accuracy > model_a_accuracy:\n",
    "        print(\"Model B (Random Forest) performs better.\")\n",
    "    else:\n",
    "        print(\"Both models perform equally.\")\n",
    "\n",
    "ab_test(lr_preds, rf_preds, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. G-Test\n",
    "\n",
    "The G-test is a statistical test that can be used to compare the performance of two models. It's similar to the chi-squared test but is more accurate for small sample sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_test(model_a_preds, model_b_preds, true_labels):\n",
    "    # Create contingency table\n",
    "    contingency_table = pd.crosstab(\n",
    "        pd.Series(true_labels, name='Actual'),\n",
    "        pd.Series(np.where(model_a_preds == model_b_preds, 'Both Correct', \n",
    "                           np.where(model_a_preds == true_labels, 'A Correct', \n",
    "                                    np.where(model_b_preds == true_labels, 'B Correct', 'Both Incorrect'))),\n",
    "                  name='Prediction')\n",
    "    )\n",
    "    \n",
    "    # Perform G-test\n",
    "    g_stat, p_value, dof, expected = chi2_contingency(contingency_table, lambda_=\"log-likelihood\")\n",
    "    \n",
    "    print(\"Contingency Table:\")\n",
    "    print(contingency_table)\n",
    "    print(f\"\\nG-statistic: {g_stat:.4f}\")\n",
    "    print(f\"p-value: {p_value:.4f}\")\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        print(\"There is a significant difference between the models.\")\n",
    "    else:\n",
    "        print(\"There is no significant difference between the models.\")\n",
    "\n",
    "g_test(lr_preds, rf_preds, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi-armed Bandit\n",
    "\n",
    "The multi-armed bandit approach is a method of dynamically allocating resources to the best-performing option while continuing to explore other options. We'll implement a simple epsilon-greedy strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_armed_bandit(model_a, model_b, X, y, n_rounds=1000, epsilon=0.1):\n",
    "    model_a_correct = 0\n",
    "    model_b_correct = 0\n",
    "    model_a_count = 0\n",
    "    model_b_count = 0\n",
    "    \n",
    "    for _ in range(n_rounds):\n",
    "        if np.random.random() < epsilon:  # Explore\n",
    "            chosen_model = np.random.choice(['A', 'B'])\n",
    "        else:  # Exploit\n",
    "            model_a_rate = model_a_correct / model_a_count if model_a_count > 0 else 0\n",
    "            model_b_rate = model_b_correct / model_b_count if model_b_count > 0 else 0\n",
    "            chosen_model = 'A' if model_a_rate >= model_b_rate else 'B'\n",
    "        \n",
    "        # Randomly select a sample\n",
    "        idx = np.random.randint(0, len(X))\n",
    "        x, true_y = X[idx:idx+1], y[idx]\n",
    "        \n",
    "        if chosen_model == 'A':\n",
    "            pred = model_a.predict(x)[0]\n",
    "            model_a_count += 1\n",
    "            if pred == true_y:\n",
    "                model_a_correct += 1\n",
    "        else:\n",
    "            pred = model_b.predict(x)[0]\n",
    "            model_b_count += 1\n",
    "            if pred == true_y:\n",
    "                model_b_correct += 1\n",
    "    \n",
    "    print(f\"Model A (Logistic Regression) accuracy: {model_a_correct / model_a_count:.4f}\")\n",
    "    print(f\"Model B (Random Forest) accuracy: {model_b_correct / model_b_count:.4f}\")\n",
    "    print(f\"Model A was chosen {model_a_count} times\")\n",
    "    print(f\"Model B was chosen {model_b_count} times\")\n",
    "\n",
    "multi_armed_bandit(lr_model, rf_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Defining a Metric-only Dataset\n",
    "\n",
    "In some cases, it's useful to define a separate dataset solely for evaluating model performance. This approach can help prevent overfitting to the test set and provide a more robust comparison between models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a new dataset for metric evaluation\n",
    "X_metric, y_metric = make_classification(n_samples=5000, n_features=20, n_classes=2, random_state=100)\n",
    "\n",
    "def evaluate_on_metric_dataset(model_a, model_b, X, y):\n",
    "    model_a_preds = model_a.predict(X)\n",
    "    model_b_preds = model_b.predict(X)\n",
    "    \n",
    "    model_a_accuracy = accuracy_score(y, model_a_preds)\n",
    "    model_b_accuracy = accuracy_score(y, model_b_preds)\n",
    "    \n",
    "    model_a_log_loss = log_loss(y, model_a.predict_proba(X))\n",
    "    model_b_log_loss = log_loss(y, model_b.predict_proba(X))\n",
    "    \n",
    "    print(f\"Model A (Logistic Regression) Accuracy: {model_a_accuracy:.4f}\")\n",
    "    print(f\"Model B (Random Forest) Accuracy: {model_b_accuracy:.4f}\")\n",
    "    print(f\"Model A (Logistic Regression) Log Loss: {model_a_log_loss:.4f}\")\n",
    "    print(f\"Model B (Random Forest) Log Loss: {model_b_log_loss:.4f}\")\n",
    "    \n",
    "    # Visualize the results\n",
    "    metrics = ['Accuracy', 'Log Loss (lower is better)']\n",
    "    model_a_scores = [model_a_accuracy, model_a_log_loss]\n",
    "    model_b_scores = [model_b_accuracy, model_b_log_loss]\n",
    "    \n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    rects1 = ax.bar(x - width/2, model_a_scores, width, label='Model A (Logistic Regression)')\n",
    "    rects2 = ax.bar(x + width/2, model_b_scores, width, label='Model B (Random Forest)')\n",
    "    \n",
    "    ax.set_ylabel('Scores')\n",
    "    ax.set_title('Model Comparison on Metric Dataset')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(metrics)\n",
    "    ax.legend()\n",
    "    \n",
    "    ax.bar_label(rects1, padding=3)\n",
    "    ax.bar_label(rects2, padding=3)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "evaluate_on_metric_dataset(lr_model, rf_model, X_metric, y_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this lecture, we've explored four different strategies for comparing machine learning models in a production environment:\n",
    "\n",
    "1. A/B Testing: A straightforward comparison of model performance on a test set.\n",
    "2. G-Test: A statistical test to determine if there's a significant difference between model performances.\n",
    "3. Multi-armed Bandit: A dynamic approach that balances exploration and exploitation to find the best-performing model.\n",
    "4. Metric-only Dataset: Using a separate dataset for model evaluation to prevent overfitting to the test set.\n",
    "\n",
    "Each approach has its strengths and is suitable for different scenarios:\n",
    "\n",
    "- A/B Testing is simple and easy to implement but may not capture the statistical significance of the difference.\n",
    "- The G-Test provides a statistical foundation for comparing models but requires more data to be reliable.\n",
    "- The Multi-armed Bandit approach is adaptive and can be useful in online learning scenarios where you want to balance exploration and exploitation.\n",
    "- Using a Metric-only Dataset provides a more robust evaluation but requires additional data and may not capture real-time changes in data distribution.\n",
    "\n",
    "When choosing a method for model comparison in production, consider factors such as:\n",
    "- The amount of data available\n",
    "- The cost of making incorrect predictions\n",
    "- The need for real-time adaptation\n",
    "- The stability of the data distribution over time\n",
    "\n",
    "Let's now explore some additional considerations and advanced techniques for model comparison in production environments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
