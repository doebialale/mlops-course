{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Model Deployment Techniques with FastAPI\n",
    "\n",
    "This tutorial demonstrates various techniques for deploying machine learning models in production environments using FastAPI. We'll cover the following deployment strategies:\n",
    "\n",
    "1. Single Deployment\n",
    "2. Silent Deployment\n",
    "3. Canary Deployment\n",
    "4. Multi-armed Bandit Deployment\n",
    "\n",
    "We'll use a simple example scenario with two models: a current model in production and a new model we want to deploy. We'll implement these deployments using FastAPI and discuss their pros and cons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import uvicorn\n",
    "import random\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation and Model Training\n",
    "\n",
    "Let's start by generating some synthetic data and training two models: a \"current\" model (Logistic Regression) and a \"new\" model (Random Forest) that we want to deploy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "X, y = make_classification(n_samples=10000, n_features=20, n_classes=2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train \"current\" model (Logistic Regression)\n",
    "current_model = LogisticRegression(random_state=42)\n",
    "current_model.fit(X_train, y_train)\n",
    "\n",
    "# Train \"new\" model (Random Forest)\n",
    "new_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "new_model.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Current model accuracy: {accuracy_score(y_test, current_model.predict(X_test)):.4f}\")\n",
    "print(f\"New model accuracy: {accuracy_score(y_test, new_model.predict(X_test)):.4f}\")\n",
    "\n",
    "# Save models\n",
    "joblib.dump(current_model, 'current_model.joblib')\n",
    "joblib.dump(new_model, 'new_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastAPI Setup\n",
    "\n",
    "Now, let's set up a basic FastAPI application and define a Pydantic model for our input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = FastAPI()\n",
    "\n",
    "class InputData(BaseModel):\n",
    "    features: list\n",
    "\n",
    "@app.get(\"/\")\n",
    "def read_root():\n",
    "    return {\"message\": \"Welcome to the ML Model Deployment API\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Single Deployment\n",
    "\n",
    "In single deployment, we replace the current model with the new model all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the new model\n",
    "model = joblib.load('new_model.joblib')\n",
    "\n",
    "@app.post(\"/predict/single\")\n",
    "def predict_single(data: InputData):\n",
    "    try:\n",
    "        features = np.array(data.features).reshape(1, -1)\n",
    "        prediction = model.predict(features)[0]\n",
    "        return {\"prediction\": int(prediction)}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=400, detail=str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this single deployment:\n",
    "\n",
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "```\n",
    "\n",
    "Pros of Single Deployment:\n",
    "- Simple to implement and understand\n",
    "- Quick to execute\n",
    "\n",
    "Cons of Single Deployment:\n",
    "- Risky - if the new model performs poorly, it affects all users immediately\n",
    "- No gradual transition period\n",
    "- Difficult to revert quickly if issues arise\n",
    "\n",
    "## 2. Silent Deployment\n",
    "\n",
    "In silent deployment, we deploy the new model alongside the current model, but only use it for logging and comparison without affecting the actual output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both models\n",
    "current_model = joblib.load('current_model.joblib')\n",
    "new_model = joblib.load('new_model.joblib')\n",
    "\n",
    "@app.post(\"/predict/silent\")\n",
    "def predict_silent(data: InputData):\n",
    "    try:\n",
    "        features = np.array(data.features).reshape(1, -1)\n",
    "        current_prediction = current_model.predict(features)[0]\n",
    "        new_prediction = new_model.predict(features)[0]\n",
    "        \n",
    "        # Log the predictions (in a real scenario, you'd use a proper logging system)\n",
    "        print(f\"Current model prediction: {current_prediction}, New model prediction: {new_prediction}\")\n",
    "        \n",
    "        # Return only the current model's prediction\n",
    "        return {\"prediction\": int(current_prediction)}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=400, detail=str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros of Silent Deployment:\n",
    "- Very safe - no impact on users\n",
    "- Allows thorough testing and comparison in the production environment\n",
    "- Provides real-world performance data for the new model\n",
    "\n",
    "Cons of Silent Deployment:\n",
    "- Requires additional computational resources\n",
    "- Delays the benefits of the new model (if it performs better)\n",
    "- May require significant logging and analysis infrastructure\n",
    "\n",
    "## 3. Canary Deployment\n",
    "\n",
    "In canary deployment, we gradually roll out the new model to a small percentage of users, increasing over time if performance is satisfactory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both models\n",
    "current_model = joblib.load('current_model.joblib')\n",
    "new_model = joblib.load('new_model.joblib')\n",
    "\n",
    "# Canary deployment settings\n",
    "canary_percentage = 0.1  # 10% of traffic goes to the new model\n",
    "\n",
    "@app.post(\"/predict/canary\")\n",
    "def predict_canary(data: InputData):\n",
    "    try:\n",
    "        features = np.array(data.features).reshape(1, -1)\n",
    "        \n",
    "        if random.random() < canary_percentage:\n",
    "            # Use new model\n",
    "            prediction = new_model.predict(features)[0]\n",
    "            model_used = \"new\"\n",
    "        else:\n",
    "            # Use current model\n",
    "            prediction = current_model.predict(features)[0]\n",
    "            model_used = \"current\"\n",
    "        \n",
    "        return {\"prediction\": int(prediction), \"model_used\": model_used}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=400, detail=str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros of Canary Deployment:\n",
    "- Reduces risk by limiting exposure to the new model\n",
    "- Allows for gradual rollout and monitoring\n",
    "- Easier to rollback if issues are detected\n",
    "\n",
    "Cons of Canary Deployment:\n",
    "- More complex to implement and manage\n",
    "- May lead to inconsistent user experiences during the transition\n",
    "- Requires careful monitoring and decision-making during the rollout process\n",
    "\n",
    "## 4. Multi-armed Bandit Deployment\n",
    "\n",
    "The multi-armed bandit approach dynamically allocates traffic to the model that performs best, continually learning and adapting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both models\n",
    "current_model = joblib.load('current_model.joblib')\n",
    "new_model = joblib.load('new_model.joblib')\n",
    "\n",
    "# Multi-armed bandit settings\n",
    "epsilon = 0.1\n",
    "current_model_correct = 0\n",
    "new_model_correct = 0\n",
    "current_model_count = 0\n",
    "new_model_count = 0\n",
    "\n",
    "@app.post(\"/predict/bandit\")\n",
    "def predict_bandit(data: InputData):\n",
    "    global current_model_correct, new_model_correct, current_model_count, new_model_count\n",
    "    \n",
    "    try:\n",
    "        features = np.array(data.features).reshape(1, -1)\n",
    "        \n",
    "        if random.random() < epsilon:\n",
    "            # Explore: randomly choose a model\n",
    "            use_new_model = random.choice([True, False])\n",
    "        else:\n",
    "            # Exploit: choose the model with the higher success rate\n",
    "            current_rate = current_model_correct / current_model_count if current_model_count > 0 else 0\n",
    "            new_rate = new_model_correct / new_model_count if new_model_count > 0 else 0\n",
    "            use_new_model = new_rate >= current_rate\n",
    "        \n",
    "        if use_new_model:\n",
    "            prediction = new_model.predict(features)[0]\n",
    "            new_model_count += 1\n",
    "            model_used = \"new\"\n",
    "        else:\n",
    "            prediction = current_model.predict(features)[0]\n",
    "            current_model_count += 1\n",
    "            model_used = \"current\"\n",
    "        \n",
    "        # In a real-world scenario, you'd update the correct predictions based on feedback\n",
    "        # Here, we're simulating it with a random choice\n",
    "        if random.random() < 0.8:  # Assuming 80% accuracy\n",
    "            if use_new_model:\n",
    "                new_model_correct += 1\n",
    "            else:\n",
    "                current_model_correct += 1\n",
    "        \n",
    "        return {\"prediction\": int(prediction), \"model_used\": model_used}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=400, detail=str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros of Multi-armed Bandit Deployment:\n",
    "- Automatically adapts to the best-performing model\n",
    "- Balances exploration of the new model with exploitation of the best-known model\n",
    "- Can handle multiple models simultaneously\n",
    "\n",
    "Cons of Multi-armed Bandit Deployment:\n",
    "- More complex to implement and understand\n",
    "- Requires a feedback mechanism to update model performance\n",
    "- May lead to inconsistent user experiences\n",
    "\n",
    "## Running the FastAPI Application\n",
    "\n",
    "To run the FastAPI application with all deployment strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the Right Deployment Strategy\n",
    "\n",
    "The choice of deployment strategy depends on various factors specific to your project and organization. Here are some guidelines:\n",
    "\n",
    "1. **Single Deployment**: Use when you're very confident in the new model's performance and can afford a short downtime. Suitable for non-critical applications or when you have a robust rollback plan.\n",
    "\n",
    "2. **Silent Deployment**: Ideal when you want to thoroughly test a new model in a production environment without any risk. Use this when you have the resources to run both models simultaneously and can afford the time for extended testing.\n",
    "\n",
    "3. **Canary Deployment**: Good for gradually introducing a new model while closely monitoring its performance. Use this when you want to limit potential negative impacts and have the capability to quickly adjust the traffic distribution or rollback if needed.\n",
    "\n",
    "4. **Multi-armed Bandit**: Best when you want to dynamically optimize model selection in real-time. Use this when you have multiple models to compare, can handle some inconsistency in user experience, and have a reliable feedback mechanism to evaluate model performance quickly.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this tutorial, we've explored four different strategies for deploying machine learning models using FastAPI:\n",
    "\n",
    "1. Single Deployment\n",
    "2. Silent Deployment\n",
    "3. Canary Deployment\n",
    "4. Multi-armed Bandit Deployment\n",
    "\n",
    "Each strategy has its own strengths and weaknesses, and the choice depends on your specific requirements, risk tolerance, and resources. By understanding these deployment techniques, you can make informed decisions about how to roll out new models in your production environment.\n",
    "\n",
    "Remember that successful deployment often involves a combination of these strategies and may require additional considerations such as:\n",
    "\n",
    "- Monitoring and logging\n",
    "- A/B testing frameworks\n",
    "- Automated rollback mechanisms\n",
    "- Performance optimization\n",
    "- Scalability and load balancing\n",
    "\n",
    "As you implement these strategies in real-world scenarios, you'll likely need to adapt and combine them to suit your specific needs. The key is to prioritize safe, controlled, and reversible deployments that allow you to confidently improve your machine learning models in production."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
