{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Ray Pipeline for Machine Learning\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates an advanced machine learning pipeline using Ray and scikit-learn. We'll build a distributed pipeline that includes:\n",
    "\n",
    "1. Distributed data loading and preprocessing\n",
    "2. Parallel feature engineering\n",
    "3. Distributed cross-validation\n",
    "4. Hyperparameter tuning with Ray Tune\n",
    "5. Distributed model training\n",
    "6. Parallel model evaluation\n",
    "7. Model serving with Ray Serve\n",
    "\n",
    "This pipeline will showcase Ray's capabilities in handling large-scale machine learning workflows efficiently.\n",
    "\n",
    "Let's start by importing the necessary libraries and initializing Ray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.serve.drivers import DAGDriver\n",
    "from ray.serve.deployments import PythonFunctionDeployment\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "# Initialize Ray\n",
    "ray.init(num_cpus=8, num_gpus=0)  # Adjust based on your machine's capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Distributed Data Loading and Preprocessing\n",
    "\n",
    "We'll start by creating a large synthetic dataset and distributing its loading and preprocessing across multiple Ray tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def generate_data_chunk(n_samples, n_features, n_classes):\n",
    "    X, y = make_classification(n_samples=n_samples, n_features=n_features, n_classes=n_classes, random_state=42)\n",
    "    return pd.DataFrame(X), pd.Series(y)\n",
    "\n",
    "@ray.remote\n",
    "def preprocess_data(X, y):\n",
    "    # Simulate some preprocessing steps\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    return X, y\n",
    "\n",
    "# Generate and preprocess data in parallel\n",
    "n_chunks = 10\n",
    "data_refs = [generate_data_chunk.remote(100000, 50, 2) for _ in range(n_chunks)]\n",
    "preprocessed_refs = [preprocess_data.remote(*chunk_ref) for chunk_ref in data_refs]\n",
    "\n",
    "# Collect results\n",
    "X_list, y_list = zip(*ray.get(preprocessed_refs))\n",
    "X = np.concatenate(X_list)\n",
    "y = np.concatenate(y_list)\n",
    "\n",
    "print(f\"Generated dataset shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parallel Feature Engineering\n",
    "\n",
    "Next, we'll perform some feature engineering tasks in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def engineer_features(X_chunk):\n",
    "    # Simulate complex feature engineering\n",
    "    new_features = np.column_stack([\n",
    "        np.mean(X_chunk, axis=1),\n",
    "        np.std(X_chunk, axis=1),\n",
    "        np.max(X_chunk, axis=1),\n",
    "        np.min(X_chunk, axis=1),\n",
    "        np.median(X_chunk, axis=1)\n",
    "    ])\n",
    "    return np.hstack([X_chunk, new_features])\n",
    "\n",
    "# Split data into chunks for parallel processing\n",
    "chunk_size = len(X) // ray.available_resources()['CPU']\n",
    "X_chunks = [X[i:i+chunk_size] for i in range(0, len(X), chunk_size)]\n",
    "\n",
    "# Engineer features in parallel\n",
    "engineered_chunks = ray.get([engineer_features.remote(chunk) for chunk in X_chunks])\n",
    "X_engineered = np.vstack(engineered_chunks)\n",
    "\n",
    "print(f\"Engineered dataset shape: {X_engineered.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Distributed Cross-Validation\n",
    "\n",
    "We'll implement a distributed cross-validation function to evaluate our model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def cross_validate_fold(X_train, y_train, X_val, y_val, params):\n",
    "    model = RandomForestClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    return accuracy_score(y_val, y_pred)\n",
    "\n",
    "def distributed_cross_validation(X, y, params, n_splits=5):\n",
    "    from sklearn.model_selection import KFold\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    tasks = []\n",
    "    for train_index, val_index in kf.split(X):\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "        tasks.append(cross_validate_fold.remote(X_train, y_train, X_val, y_val, params))\n",
    "    \n",
    "    scores = ray.get(tasks)\n",
    "    return np.mean(scores)\n",
    "\n",
    "# Example usage\n",
    "params = {'n_estimators': 100, 'max_depth': 10}\n",
    "cv_score = distributed_cross_validation(X_engineered, y, params)\n",
    "print(f\"Cross-validation score: {cv_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Tuning with Ray Tune\n",
    "\n",
    "Now, we'll use Ray Tune to perform distributed hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(config):\n",
    "    score = distributed_cross_validation(X_engineered, y, config)\n",
    "    tune.report(mean_accuracy=score)\n",
    "\n",
    "search_space = {\n",
    "    \"n_estimators\": tune.choice([50, 100, 200]),\n",
    "    \"max_depth\": tune.choice([5, 10, 15, 20]),\n",
    "    \"min_samples_split\": tune.choice([2, 5, 10]),\n",
    "    \"min_samples_leaf\": tune.choice([1, 2, 4])\n",
    "}\n",
    "\n",
    "analysis = tune.run(\n",
    "    objective,\n",
    "    config=search_space,\n",
    "    num_samples=20,\n",
    "    scheduler=ASHAScheduler(metric=\"mean_accuracy\", mode=\"max\"),\n",
    "    resources_per_trial={\"cpu\": 2}\n",
    ")\n",
    "\n",
    "best_config = analysis.get_best_config(metric=\"mean_accuracy\", mode=\"max\")\n",
    "print(\"Best hyperparameters found:\", best_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Distributed Model Training\n",
    "\n",
    "With the best hyperparameters, we'll now train our model using distributed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class DistributedRandomForest:\n",
    "    def __init__(self, n_estimators, max_depth, min_samples_split, min_samples_leaf):\n",
    "        self.model = RandomForestClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf\n",
    "        )\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Split data for training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_engineered, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create distributed model\n",
    "dist_rf = DistributedRandomForest.remote(**best_config)\n",
    "\n",
    "# Train in parallel\n",
    "chunk_size = len(X_train) // ray.available_resources()['CPU']\n",
    "train_tasks = []\n",
    "for i in range(0, len(X_train), chunk_size):\n",
    "    X_chunk = X_train[i:i+chunk_size]\n",
    "    y_chunk = y_train[i:i+chunk_size]\n",
    "    train_tasks.append(dist_rf.fit.remote(X_chunk, y_chunk))\n",
    "\n",
    "# Wait for training to complete\n",
    "ray.get(train_tasks)\n",
    "print(\"Distributed training completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Parallel Model Evaluation\n",
    "\n",
    "Now that our model is trained, let's evaluate it using parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def evaluate_chunk(model, X_chunk, y_chunk):\n",
    "    y_pred = model.predict.remote(X_chunk)\n",
    "    return ray.get(y_pred), y_chunk\n",
    "\n",
    "# Evaluate in parallel\n",
    "chunk_size = len(X_test) // ray.available_resources()['CPU']\n",
    "eval_tasks = []\n",
    "for i in range(0, len(X_test), chunk_size):\n",
    "    X_chunk = X_test[i:i+chunk_size]\n",
    "    y_chunk = y_test[i:i+chunk_size]\n",
    "    eval_tasks.append(evaluate_chunk.remote(dist_rf, X_chunk, y_chunk))\n",
    "\n",
    "# Collect results\n",
    "results = ray.get(eval_tasks)\n",
    "y_pred, y_true = zip(*results)\n",
    "y_pred = np.concatenate(y_pred)\n",
    "y_true = np.concatenate(y_true)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Serving with Ray Serve\n",
    "\n",
    "Finally, let's deploy our trained model using Ray Serve for real-time predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import serve\n",
    "\n",
    "@serve.deployment(route_prefix=\"/predict\")\n",
    "class ModelServer:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    async def __call__(self, request):\n",
    "        data = await request.json()\n",
    "        features = np.array(data['features']).reshape(1, -1)\n",
    "        prediction = await self.model.predict.remote(features)\n",
    "        return {'prediction': int(ray.get(prediction)[0])}\n",
    "\n",
    "# Deploy the model\n",
    "serve.start()\n",
    "ModelServer.deploy(model=dist_rf)\n",
    "\n",
    "print(\"Model deployed and ready to serve predictions.\")\n",
    "# Simulate a prediction request\n",
    "import requests\n",
    "import json\n",
    "sample_input = X_test[0].tolist()\n",
    "response = requests.post(\"http://localhost:8000/predict\", json={\"features\": sample_input})\n",
    "print(f'Prediction for sample input: {response.json()['prediction']}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this final section, we've deployed our trained model using Ray Serve. This allows us to make real-time predictions using a simple HTTP API. The `ModelServer` class wraps our distributed random forest model and handles incoming prediction requests.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This notebook has demonstrated an advanced machine learning pipeline using Ray and scikit-learn. We've covered several key aspects of distributed machine learning:\n",
    "\n",
    "1. **Distributed Data Loading and Preprocessing**: We generated and preprocessed a large dataset in parallel, showcasing Ray's ability to handle big data efficiently.\n",
    "\n",
    "2. **Parallel Feature Engineering**: We performed feature engineering tasks across multiple workers, demonstrating how Ray can speed up computationally intensive tasks.\n",
    "\n",
    "3. **Distributed Cross-Validation**: We implemented a custom distributed cross-validation function, allowing for faster and more scalable model evaluation.\n",
    "\n",
    "4. **Hyperparameter Tuning with Ray Tune**: We used Ray Tune to perform distributed hyperparameter optimization, efficiently searching the parameter space.\n",
    "\n",
    "5. **Distributed Model Training**: We created a distributed version of RandomForestClassifier and trained it in parallel across multiple workers.\n",
    "\n",
    "6. **Parallel Model Evaluation**: We evaluated our trained model on the test set using parallel processing for faster results.\n",
    "\n",
    "7. **Model Serving with Ray Serve**: Finally, we deployed our trained model as a scalable prediction service using Ray Serve.\n",
    "\n",
    "This pipeline demonstrates how Ray can be used to scale up machine learning workflows, from data preprocessing to model deployment. By leveraging distributed computing, we can handle larger datasets, perform more extensive hyperparameter searches, and speed up model training and evaluation.\n",
    "\n",
    "In an MLOps context, this pipeline showcases several important capabilities:\n",
    "\n",
    "- **Scalability**: The ability to handle large datasets and complex computations by distributing work across multiple machines or cores.\n",
    "- **Flexibility**: Ray's API allows for easy parallelization of existing scikit-learn code with minimal changes.\n",
    "- **End-to-End Workflow**: From data preprocessing to model serving, Ray provides tools for each step of the ML lifecycle.\n",
    "- **Resource Efficiency**: By dynamically allocating tasks to available resources, Ray helps utilize cluster resources effectively.\n",
    "\n",
    "To further enhance this pipeline for production use, consider integrating it with other MLOps tools:\n",
    "\n",
    "- Use MLflow or Weights & Biases for experiment tracking and model versioning.\n",
    "- Implement data version control using DVC or other data versioning tools.\n",
    "- Set up continuous integration and deployment (CI/CD) pipelines for automating the model update process.\n",
    "- Implement monitoring and logging for the deployed model to track performance and detect drift.\n",
    "\n",
    "By combining Ray with these MLOps practices, you can create robust, scalable, and maintainable machine learning systems capable of handling real-world challenges."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
