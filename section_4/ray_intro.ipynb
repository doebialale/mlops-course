{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Ray for Distributed Computing in Python\n",
    "\n",
    "## Overview\n",
    "\n",
    "Ray is an open-source framework for scaling Python applications. It provides a simple, universal API for building distributed applications. Ray is particularly useful in machine learning and artificial intelligence workflows, where it can help parallelize computations and manage distributed resources efficiently.\n",
    "\n",
    "In this lecture, we'll cover:\n",
    "\n",
    "1. Basic Ray concepts\n",
    "2. Setting up Ray\n",
    "3. Remote functions and parallel execution\n",
    "4. Ray Tasks vs. Actors\n",
    "5. Shared memory and object stores\n",
    "6. Ray for Machine Learning\n",
    "7. Best practices and considerations\n",
    "\n",
    "Let's begin by installing Ray and importing necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ray\n",
    "\n",
    "import ray\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Ray Concepts\n",
    "\n",
    "Ray's core abstraction is a task - a remote function invocation. Ray uses these tasks to distribute computation across a cluster of machines. The key components of Ray are:\n",
    "\n",
    "- **Workers**: Python processes that execute tasks.\n",
    "- **Drivers**: The main Python script that defines and invokes remote tasks.\n",
    "- **Object Store**: A distributed shared-memory object store.\n",
    "- **Scheduler**: Assigns tasks to workers.\n",
    "\n",
    "## 2. Setting up Ray\n",
    "\n",
    "To use Ray, we first need to initialize it. This can be done locally or on a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Ray\n",
    "ray.init()\n",
    "\n",
    "# You can also specify resources\n",
    "# ray.init(num_cpus=4, num_gpus=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Remote Functions and Parallel Execution\n",
    "\n",
    "Ray allows you to execute functions remotely using the `@ray.remote` decorator. These functions can run in parallel on different machines or cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def slow_function(i):\n",
    "    time.sleep(1)  # Simulate a slow operation\n",
    "    return i * i\n",
    "\n",
    "# Execute functions in parallel\n",
    "start_time = time.time()\n",
    "results = ray.get([slow_function.remote(i) for i in range(4)])\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Results: {results}\")\n",
    "print(f\"Time taken: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we define a `slow_function` that simulates a time-consuming operation. By using `@ray.remote`, we can execute multiple instances of this function in parallel, significantly reducing the total execution time.\n",
    "\n",
    "## 4. Ray Tasks vs. Actors\n",
    "\n",
    "Ray provides two main abstractions for parallel computation:\n",
    "\n",
    "1. **Tasks**: Stateless functions (like we saw above)\n",
    "2. **Actors**: Stateful workers\n",
    "\n",
    "Let's look at an example using an Actor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class Counter:\n",
    "    def __init__(self):\n",
    "        self.value = 0\n",
    "    \n",
    "    def increment(self):\n",
    "        self.value += 1\n",
    "        return self.value\n",
    "\n",
    "# Create an actor\n",
    "counter = Counter.remote()\n",
    "\n",
    "# Increment the counter in parallel\n",
    "results = ray.get([counter.increment.remote() for _ in range(5)])\n",
    "print(f\"Counter values: {results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we create a `Counter` actor that maintains its state across method calls. This is useful for scenarios where you need to maintain state in a distributed setting.\n",
    "\n",
    "## 5. Shared Memory and Object Stores\n",
    "\n",
    "Ray uses a distributed object store to efficiently pass large objects between tasks. This is particularly useful for machine learning workloads with large datasets or models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a large object\n",
    "large_matrix = np.random.rand(1000, 1000)\n",
    "\n",
    "# Put the object in the object store\n",
    "matrix_id = ray.put(large_matrix)\n",
    "\n",
    "@ray.remote\n",
    "def matrix_sum(matrix):\n",
    "    return np.sum(matrix)\n",
    "\n",
    "# Use the object reference in a task\n",
    "result = ray.get(matrix_sum.remote(matrix_id))\n",
    "print(f\"Sum of matrix elements: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using `ray.put()`, we can efficiently share large objects between tasks without the need for serialization and deserialization.\n",
    "\n",
    "## 6. Ray for Machine Learning\n",
    "\n",
    "Ray provides several libraries specifically designed for machine learning workloads:\n",
    "\n",
    "- **Ray Tune**: For hyperparameter tuning\n",
    "- **Ray Train**: For distributed model training\n",
    "- **Ray Serve**: For model serving\n",
    "\n",
    "Let's look at a simple example using Ray Tune for hyperparameter optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "def objective(config):\n",
    "    # Simulate a model training process\n",
    "    for step in range(100):\n",
    "        intermediate_score = config[\"a\"] * step + config[\"b\"]\n",
    "        tune.report(score=intermediate_score)\n",
    "\n",
    "analysis = tune.run(\n",
    "    objective,\n",
    "    config={\n",
    "        \"a\": tune.uniform(0, 1),\n",
    "        \"b\": tune.uniform(0, 20)\n",
    "    },\n",
    "    num_samples=10,\n",
    "    scheduler=ASHAScheduler(metric=\"score\", mode=\"max\")\n",
    ")\n",
    "\n",
    "print(\"Best config:\", analysis.get_best_config(metric=\"score\", mode=\"max\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates how Ray Tune can be used to optimize hyperparameters in a distributed manner.\n",
    "\n",
    "## 7. Best Practices and Considerations\n",
    "\n",
    "When using Ray, keep the following best practices in mind:\n",
    "\n",
    "1. **Task Granularity**: Ensure tasks are not too small (overhead of distribution) or too large (limits parallelism).\n",
    "2. **Resource Management**: Specify CPU and GPU requirements for tasks and actors when necessary.\n",
    "3. **Error Handling**: Use Ray's built-in retry mechanisms for fault tolerance.\n",
    "4. **Monitoring**: Utilize Ray's dashboard for cluster monitoring and debugging.\n",
    "5. **Data Transfer**: Minimize data transfer between nodes by using Ray's object store effectively.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Ray provides a powerful framework for distributed computing in Python, with particular strengths in machine learning workflows. Its simple API allows for easy parallelization of existing code, while its specialized libraries offer advanced functionality for ML tasks.\n",
    "\n",
    "In an MLOps context, Ray can be particularly useful for:\n",
    "- Distributed data preprocessing\n",
    "- Parallel model training\n",
    "- Hyperparameter tuning at scale\n",
    "- Distributed inference\n",
    "\n",
    "By integrating Ray into your MLOps pipeline, you can significantly improve the scalability and efficiency of your machine learning workflows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}